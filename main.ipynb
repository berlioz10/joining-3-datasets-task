{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = defaultdict(str, phone=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading facebook_dataset.csv... done\n",
      "Reading google_dataset.csv... done\n",
      "Reading website_dataset.csv... done\n"
     ]
    }
   ],
   "source": [
    "dfs = [None] * 3\n",
    "csvs = [\"facebook_dataset.csv\", \"google_dataset.csv\", \"website_dataset.csv\"]\n",
    "seps = [\",\", \",\", \";\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub('([^,\\n])\"{2,3}', '\\1\"', text)\n",
    "\n",
    "for i, csv_name in enumerate(csvs):\n",
    "    with open(csv_name, 'r') as file:\n",
    "        # each csv contains \\\", so we have to save it\n",
    "        data = file.read().replace('\\\\\"', 'PLACEHOLDER_FOR_QUOTE')\n",
    "        # these cleanup is needed because of website_dataset\n",
    "        data = clean_text(data)\n",
    "    csv_data = StringIO(data)\n",
    "\n",
    "    dfs[i] = pd.read_csv(csv_data, sep=seps[i], dtype=types)\n",
    "    dfs[i].replace('PLACEHOLDER_FOR_QUOTE', \"\\\"\", inplace=True)\n",
    "    print(\"Reading \" + csv_name + \"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook csv info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72010 entries, 0 to 72009\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   domain              72010 non-null  object\n",
      " 1   address             57380 non-null  object\n",
      " 2   categories          55310 non-null  object\n",
      " 3   city                45106 non-null  object\n",
      " 4   country_code        57874 non-null  object\n",
      " 5   country_name        45404 non-null  object\n",
      " 6   description         28003 non-null  object\n",
      " 7   email               20289 non-null  object\n",
      " 8   link                72010 non-null  object\n",
      " 9   name                72009 non-null  object\n",
      " 10  page_type           72004 non-null  object\n",
      " 11  phone               44866 non-null  object\n",
      " 12  phone_country_code  38013 non-null  object\n",
      " 13  region_code         45101 non-null  object\n",
      " 14  region_name         45101 non-null  object\n",
      " 15  zip_code            36049 non-null  object\n",
      "dtypes: object(16)\n",
      "memory usage: 8.8+ MB\n",
      "\n",
      "google csv info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 356520 entries, 0 to 356519\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   address             330552 non-null  object\n",
      " 1   category            304870 non-null  object\n",
      " 2   city                311002 non-null  object\n",
      " 3   country_code        304084 non-null  object\n",
      " 4   country_name        311039 non-null  object\n",
      " 5   name                356489 non-null  object\n",
      " 6   phone               323948 non-null  object\n",
      " 7   phone_country_code  241258 non-null  object\n",
      " 8   raw_address         311875 non-null  object\n",
      " 9   raw_phone           328160 non-null  object\n",
      " 10  region_code         310849 non-null  object\n",
      " 11  region_name         310854 non-null  object\n",
      " 12  text                353005 non-null  object\n",
      " 13  zip_code            273400 non-null  object\n",
      " 14  domain              356520 non-null  object\n",
      "dtypes: object(15)\n",
      "memory usage: 40.8+ MB\n",
      "\n",
      "website csv info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72018 entries, 0 to 72017\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   root_domain    72017 non-null  object\n",
      " 1   domain_suffix  71900 non-null  object\n",
      " 2   language       66826 non-null  object\n",
      " 3   legal_name     31989 non-null  object\n",
      " 4   main_city      60966 non-null  object\n",
      " 5   main_country   64869 non-null  object\n",
      " 6   main_region    60933 non-null  object\n",
      " 7   phone          65640 non-null  object\n",
      " 8   site_name      68714 non-null  object\n",
      " 9   tld            71757 non-null  object\n",
      " 10  s_category     70634 non-null  object\n",
      "dtypes: object(11)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "facebook_df = dfs[0]\n",
    "google_df = dfs[1]\n",
    "website_df = dfs[2]\n",
    "print(\"facebook csv info:\\n\")\n",
    "facebook_df.info()\n",
    "print(\"\\ngoogle csv info:\\n\")\n",
    "google_df.info()\n",
    "print(\"\\nwebsite csv info:\\n\")\n",
    "website_df.info();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that interest us the most are Category, Address (country, region...), Phone, Company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook company unique elements:\n",
      "\n",
      "0                                                      NaN\n",
      "1        Appliance Repair & Maintenance|Home Builders &...\n",
      "2              Boats & Yachts Dealers|Boat Tours & Cruises\n",
      "3        Architects & Architectural Services|Other Engi...\n",
      "5        Other schools|High Schools|Community Center|Ad...\n",
      "                               ...                        \n",
      "71915    Auto Services|Auto Parts Store|Sporting Goods ...\n",
      "71923    Parts & Accessories|Digital & Marketing Agenci...\n",
      "71929    Community Center|CPR Training|Adult Education ...\n",
      "71946    Security Guards & Patrol Services|Social Servi...\n",
      "71953    Garden Equipment & Supplies|Landscaping & Lawn...\n",
      "Name: categories, Length: 10185, dtype: object\n",
      "\n",
      "Google company unique elements:\n",
      "\n",
      "0                   Fabric-Based Home Goods\n",
      "1                               Book Stores\n",
      "2         Other Building Material Retailers\n",
      "3                   Plastic Surgery Clinics\n",
      "4                       Catering & Delivery\n",
      "                        ...                \n",
      "149453                      Modeling Agency\n",
      "155707          Railroad Equipment & Trains\n",
      "174176               Domestic Violence NGOs\n",
      "178227            Construction Associations\n",
      "185789                     Skydiving Center\n",
      "Name: category, Length: 471, dtype: object\n",
      "\n",
      "Website company unique elements:\n",
      "\n",
      "0                                       NaN\n",
      "1             Shoes & Other Footwear Stores\n",
      "2                    Real Estate Developers\n",
      "3        Automobile Dealers & Manufacturers\n",
      "4                       Business Consulting\n",
      "                        ...                \n",
      "20510               Veterinary Associations\n",
      "21801          Glaziers & Glass Contractors\n",
      "25361                          Endodontists\n",
      "26426                             Sex Shops\n",
      "27022                      Escrow Companies\n",
      "Name: s_category, Length: 563, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cat_f = facebook_df[\"categories\"].drop_duplicates()\n",
    "print(\"Facebook company unique elements:\\n\")\n",
    "print(cat_f)\n",
    "cat_g = google_df[\"category\"].drop_duplicates()\n",
    "print(\"\\nGoogle company unique elements:\\n\")\n",
    "print(cat_g)\n",
    "cat_w = website_df[\"s_category\"].drop_duplicates()\n",
    "print(\"\\nWebsite company unique elements:\\n\")\n",
    "print(cat_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook csv has a different parser on categories, for that we will create a new list for finding out the categories from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook_cat_list = set([a for b in cat_f.str.split('|').tolist()[1:] for a in b])\n",
    "len(facebook_cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 92\n",
      "406 157\n",
      "406 65\n"
     ]
    }
   ],
   "source": [
    "# categories from google minus categories from website\n",
    "a = [x for x in cat_g if x not in cat_w]\n",
    "print(len(a), len(cat_g), len(cat_w) - len(cat_g))\n",
    "a = [x for x in facebook_cat_list if x not in cat_w]\n",
    "print(len(a), len(facebook_cat_list), len(cat_w) - len(facebook_cat_list))\n",
    "a = [x for x in facebook_cat_list if x not in cat_g]\n",
    "print(len(a), len(facebook_cat_list), len(cat_g) - len(facebook_cat_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the substractions of the categories list, we understand that:\n",
    " - google csv and facebook csv have less categories than website csv, but at least all the categories from google and facebook csv part can be located in the categories of teh website csv \n",
    " - same for facebook csv and google csv, whereas google csv has more categories than facebook csv, but still facebook csv categories can be found in all of the other csvs\n",
    " - the intersection between all of the categories is facebook csv categories, and most of them are found in website csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will analyse the domain of each company:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook company domains:\n",
      "\n",
      "72010 72010 72010\n",
      "\n",
      "Google company domains:\n",
      "\n",
      "356520 356520 72010\n",
      "\n",
      "Website company domains:\n",
      "\n",
      "72018 72018 72018\n"
     ]
    }
   ],
   "source": [
    "dom_f = facebook_df[\"domain\"]\n",
    "print(\"Facebook company domains:\\n\")\n",
    "print(len(facebook_df), len(dom_f), len(dom_f.drop_duplicates()))\n",
    "dom_g = google_df[\"domain\"]\n",
    "print(\"\\nGoogle company domains:\\n\")\n",
    "print(len(google_df), len(dom_g), len(dom_g.drop_duplicates()))\n",
    "dom_w = website_df[\"root_domain\"]\n",
    "print(\"\\nWebsite company domains:\\n\")\n",
    "print(len(website_df), len(dom_w), len(dom_w.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15291"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dom_g[dom_g.duplicated()].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook and Website csvs are fine, but Google csv has a lot of websites in which the domain is instagram or other well known websites. In total, there are 15000 websites which are used more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 -8\n",
      "0 0\n",
      "0 0\n",
      "13 ['MARTIN-LAFLAMM\\x01', 'Funeral Hom\\x01', ' Mobile Pet Salon - Edmonto\\x01', 'YACHTS IN\\x01', 'Fitzwilliam NH 03447', '603.585.300\\x01', 'Avenida Food Hall & Fresh Marke\\x01', 'Paint & ', 'Decor Inc\\x01', nan, '(519) 771-850\\x01', \"Let's Be Crystal Clea\\x01\", 'Â\\x01']\n"
     ]
    }
   ],
   "source": [
    "dom_w_to_list = dom_w.tolist()\n",
    "dom_g_to_list = dom_g.tolist()\n",
    "dom_f_to_list = dom_f.tolist()\n",
    "a = [x for x in dom_f_to_list if x not in dom_w_to_list]\n",
    "print(len(a), len(dom_f) - len(dom_w))\n",
    "\n",
    "a = [x for x in dom_f_to_list if x not in dom_g_to_list]\n",
    "print(len(a), len(dom_f) - len(dom_g.drop_duplicates()))\n",
    "\n",
    "a = [x for x in dom_g_to_list if x not in dom_f_to_list]\n",
    "print(len(a), len(dom_f) - len(dom_g.drop_duplicates()))\n",
    "\n",
    "a = [x for x in dom_w_to_list if x not in dom_f_to_list]\n",
    "print(len(a), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that all the Facebook and Google domains (only unique) are the same. Website csv brings 13 different domains in addition, which all seems to be wrong.\n",
    "\n",
    "Lets take into consideration as well the name of each company, as Google csv does not have unique domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(google_df[google_df[\"name\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook company domains:\n",
      "\n",
      "72010 72009 71837\n",
      "\n",
      "Google company domains:\n",
      "\n",
      "356520 356489 353898\n",
      "\n",
      "Website company domains:\n",
      "\n",
      "72018 68714 67541\n"
     ]
    }
   ],
   "source": [
    "name_f = facebook_df[\"name\"].dropna()\n",
    "print(\"Facebook company domains:\\n\")\n",
    "print(len(facebook_df), len(name_f), len(name_f.drop_duplicates()))\n",
    "name_g = google_df[\"name\"].dropna()\n",
    "print(\"\\nGoogle company domains:\\n\")\n",
    "print(len(google_df), len(name_g), len(name_g.drop_duplicates()))\n",
    "name_w = website_df[\"site_name\"].dropna()\n",
    "print(\"\\nWebsite company domains:\\n\")\n",
    "print(len(website_df), len(name_w), len(name_w.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4756                               Chorus\n",
       "7504                         Clean Freaks\n",
       "7611     Ministerio Público de la Defensa\n",
       "7810                           Volkswagen\n",
       "9882                                  JLL\n",
       "                       ...               \n",
       "71006                       Grace Academy\n",
       "71330                              Nestlé\n",
       "71331                Methodist Day School\n",
       "71492                       City of Perth\n",
       "71716                            Michelin\n",
       "Name: name, Length: 172, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_f[name_f.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47967 3295\n",
      "40527 -284480\n",
      "44540 -3295\n"
     ]
    }
   ],
   "source": [
    "name_f_to_list = name_f.tolist()\n",
    "name_g_to_list = name_g.tolist()\n",
    "name_w_to_list = name_w.tolist()\n",
    "a = [x for x in name_f_to_list if x not in name_w_to_list]\n",
    "print(len(a), len(name_f) - len(name_w))\n",
    "\n",
    "a = [x for x in name_f_to_list if x not in name_g_to_list]\n",
    "print(len(a), len(name_f) - len(name_g))\n",
    "\n",
    "a = [x for x in name_w_to_list if x not in name_f_to_list]\n",
    "print(len(a), len(name_w) - len(name_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the name column of each csv, we can observe more duplicates, and also some of them \n",
    "\n",
    "Now, for the last part which could be a better representation for uniqueness of a company: phone number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook company phone numbers:\n",
      "\n",
      "72010 44866 44787\n",
      "\n",
      "Google company numbers:\n",
      "\n",
      "356520 323948 285821\n",
      "\n",
      "Website company numbers:\n",
      "\n",
      "72018 65640 60053\n"
     ]
    }
   ],
   "source": [
    "phone_number_f = facebook_df[\"phone\"].dropna()\n",
    "print(\"Facebook company phone numbers:\\n\")\n",
    "print(len(facebook_df), len(phone_number_f), len(phone_number_f.drop_duplicates()))\n",
    "phone_number_g = google_df[\"phone\"].dropna()\n",
    "print(\"\\nGoogle company numbers:\\n\")\n",
    "print(len(google_df), len(phone_number_g), len(phone_number_g.drop_duplicates()))\n",
    "phone_number_w = website_df[\"phone\"].dropna()\n",
    "print(\"\\nWebsite company numbers:\\n\")\n",
    "print(len(website_df), len(phone_number_w), len(phone_number_w.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4273     +18009525210\n",
       "5328     +14032735683\n",
       "12420    +17809106688\n",
       "12614    +19056909209\n",
       "13835    +18173294161\n",
       "             ...     \n",
       "66486    +15167466585\n",
       "68226    +17783981111\n",
       "69436    +17057894345\n",
       "70294    +14033200383\n",
       "71278    +14506803287\n",
       "Name: phone, Length: 79, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook_df[facebook_df[\"phone\"].duplicated() & facebook_df[\"phone\"].notna()][\"phone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29458\n",
      "6838\n",
      "15765\n"
     ]
    }
   ],
   "source": [
    "def remove_plus_spaces_lines(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.replace(\"+\", \"\").replace(\" \", \"\").replace(\"-\", \"\").replace('(', \"\").replace(')', \"\")\n",
    "\n",
    "f = phone_number_f.drop_duplicates().map(remove_plus_spaces_lines).tolist()\n",
    "w = phone_number_w.drop_duplicates().map(remove_plus_spaces_lines).tolist()\n",
    "g = phone_number_g.drop_duplicates().map(remove_plus_spaces_lines).tolist()\n",
    "a = [x for x in w if x not in f]\n",
    "print(len(a))\n",
    "a = [x for x in f if x not in g]\n",
    "print(len(a))\n",
    "a = [x for x in w if x not in g]\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the phone numbers differ a lot, except facebook - google, or maybe there is a problem from their format and removing spaces, lines, paranthesis and the + from the beginning is not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving some good details about each column, now we can finally have an approach on the problem:\n",
    "Unfortunately, this problem cannot be solved with a simple \"join\" between the columns, as we have to check the other details for each company.\n",
    "For the beginning, I consider that the domain column, even if it is not wanted, it is really a great approach for the beginning, due to no duplicates except google and we can finish really fast with 70 000 data. We will prefer the category of facebook, because it is a much better representation of the data. For the beginnning, we will add only the ones from the intersection between google and facebook. In addition, we will add a frequency array for choosing which attribute is for the best if there are conflicts in names, phone numbers etc. If frequeencies are equal, we will check which one is more unique.\n",
    "\n",
    "For the other 13 domains from website csv, they won't be used because they look a bit corrupted, and from my perspective, I should not work with them until I talk with someone about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_dom = dom_g[dom_g.duplicated()].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=[\"Category\", \"Country\", \"Region\", \"Phone Number\", \"Name\", \"Domain\"])\n",
    "dummy_website_df = pd.DataFrame([[pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA]], columns=[\"s_category\", \"main_country\", \"main_region\", \"phone\", \"site_name\", \"root_domain\"])\n",
    "dummy_google_df = pd.DataFrame([[pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA]], columns=[\"category\", \"country_name\", \"region_name\", \"phone\", \"name\", \"domain\"])\n",
    "dummy_facebook_df = pd.DataFrame([[pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA]], columns=[\"categories\", \"country_name\", \"region_name\", \"phone\", \"name\", \"domain\"])\n",
    "# we will need all of the repeated domains to make sure that the companies from facebook and website csv are the same as the ones from google\n",
    "\n",
    "def create_freq_dict(row1, row2, row3, key1, key2, key3):\n",
    "    my_dict = {}    \n",
    "    if pd.notna(row1[key1]):\n",
    "        my_dict[row1[key1].lower()] = my_dict.get(row1[key1].lower(), 0) + 1\n",
    "    if pd.notna(row2[key2]):\n",
    "        my_dict[row2[key2].lower()] = my_dict.get(row2[key2].lower(), 0) + 1\n",
    "    if pd.notna(row3[key3]):\n",
    "        my_dict[row3[key3].lower()] = my_dict.get(row3[key3].lower(), 0) + 1\n",
    "\n",
    "    return my_dict\n",
    "\n",
    "for el in dom_f:\n",
    "    website_row = website_df[website_df[\"root_domain\"] == str(el)]\n",
    "    facebook_row = facebook_df[facebook_df[\"domain\"] == str(el)]\n",
    "    google_row = google_df[google_df[\"domain\"] == str(el)]\n",
    "    if len(website_row) == 0:\n",
    "        website_row = dummy_website_df\n",
    "    if len(google_row) == 0:\n",
    "        google_row = dummy_google_df\n",
    "    new_row = [\"\"] * 5 + [str(el)]\n",
    "\n",
    "    if str(el) in duplicated_dom:\n",
    "        continue\n",
    "    else:\n",
    "        # for category\n",
    "        facebook_row = facebook_row.iloc[0]\n",
    "        google_row = google_row.iloc[0]\n",
    "        website_row = website_row.iloc[0]\n",
    "        new_row[0] = str(facebook_row[\"categories\"]) \\\n",
    "            if pd.notna(facebook_row[\"categories\"]) else str(website_row[\"s_category\"]) \\\n",
    "            if pd.notna(website_row[\"s_category\"]) else str(google_row[\"category\"])\n",
    "        if pd.isna(website_row[\"s_category\"]) and str(website_row[\"s_category\"]) not in new_row[0]:\n",
    "            new_row[0] += \"|\" + str(website_row[\"s_category\"])\n",
    "        if pd.notna(google_row[\"category\"]) and str(google_row[\"category\"]) not in new_row[0]:\n",
    "            new_row[0] += \"|\" + str(google_row[\"category\"])\n",
    "        # for country, region, phone number and name\n",
    "        country_freq = create_freq_dict(website_row, facebook_row, google_row, \"main_country\", \"country_name\", \"country_name\")\n",
    "        region_freq = create_freq_dict(website_row, facebook_row, google_row, \"main_region\", \"region_name\", \"region_name\")\n",
    "        google_row[\"new_phone\"] = remove_plus_spaces_lines(google_row[\"phone\"])\n",
    "        facebook_row[\"new_phone\"] = remove_plus_spaces_lines(facebook_row[\"phone\"])\n",
    "        website_row[\"new_phone\"] = remove_plus_spaces_lines(website_row[\"phone\"])\n",
    "        phone_number_freq = create_freq_dict(website_row, facebook_row, google_row, \"new_phone\", \"new_phone\", \"new_phone\")\n",
    "        name_freq = create_freq_dict(website_row, facebook_row, google_row, \"site_name\", \"name\", \"name\")\n",
    "        \n",
    "        new_row[1] = max(country_freq, key=country_freq.get) if len(country_freq) != 0 else \"\"\n",
    "        new_row[2] = max(region_freq, key=region_freq.get) if len(region_freq) != 0 else \"\"\n",
    "        new_row[3] = max(phone_number_freq, key=phone_number_freq.get) if len(phone_number_freq) != 0 else \"\"\n",
    "        if google_row[\"new_phone\"] == new_row[3]:\n",
    "            new_row[3] = google_row[\"phone\"]\n",
    "        elif facebook_row[\"new_phone\"] == new_row[3]:\n",
    "            new_row[3] = facebook_row[\"phone\"]\n",
    "        elif website_row[\"new_phone\"] == new_row[3]:\n",
    "            new_row[3] = website_row[\"phone\"]\n",
    "\n",
    "        new_row[4] = max(name_freq, key=name_freq.get) if len(name_freq) != 0 else \"\"\n",
    "        \n",
    "        if pd.notna(google_row[\"name\"]):\n",
    "            if google_row[\"name\"].lower() == new_row[4]:\n",
    "                new_row[4] = google_row[\"name\"]\n",
    "            elif pd.notna(facebook_row[\"name\"]):\n",
    "                if facebook_row[\"name\"].lower() == new_row[4]:\n",
    "                    new_row[4] = facebook_row[\"phone\"]\n",
    "                elif pd.notna(website_row[\"site_name\"]):\n",
    "                    if website_row[\"site_name\"] == new_row[4]:\n",
    "                        new_row[4] = website_row[\"site_name\"]\n",
    "        \n",
    "\n",
    "    new_df = pd.concat([pd.DataFrame([new_row], columns=new_df.columns), new_df], ignore_index=True)\n",
    "\n",
    "new_df.to_csv(\"merged_datasets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we finished with the most obvious ones with unique values in all of our tables (from the perspective of the domain's column), we will try to identify the ones which have common domain from the facebook and website dataset. we will make an order with most similar: phone number (after parsing it) and name (at least contain the letters from it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_line(website_row, facebook_row, google_row, new_row, new_df, prioritize_website=False):\n",
    "    if prioritize_website:\n",
    "        if (pd.isna(google_row[\"domain\"])) or validate_rows_are_similar(facebook_row, google_row, \"name\", \"name\", \"phone\", \"phone\") > 0:\n",
    "            google_row = dummy_google_df.iloc[0]\n",
    "            facebook_row = dummy_facebook_df.iloc[0]\n",
    "    new_row[0] = str(facebook_row[\"categories\"]) \\\n",
    "        if pd.notna(facebook_row[\"categories\"]) else str(website_row[\"s_category\"]) \\\n",
    "        if pd.notna(website_row[\"s_category\"]) else str(google_row[\"category\"])\n",
    "    if pd.isna(website_row[\"s_category\"]) and str(website_row[\"s_category\"]) not in new_row[0]:\n",
    "        new_row[0] += \"|\" + str(website_row[\"s_category\"])\n",
    "    if pd.notna(google_row[\"category\"]) and str(google_row[\"category\"]) not in new_row[0]:\n",
    "        new_row[0] += \"|\" + str(google_row[\"category\"])\n",
    "    country_freq = create_freq_dict(website_row, facebook_row, google_row, \"main_country\", \"country_name\", \"country_name\")\n",
    "    region_freq = create_freq_dict(website_row, facebook_row, google_row, \"main_region\", \"region_name\", \"region_name\")\n",
    "    google_row[\"new_phone\"] = remove_plus_spaces_lines(google_row[\"phone\"]) if pd.notna(google_row[\"phone\"]) else \"\"\n",
    "    facebook_row[\"new_phone\"] = remove_plus_spaces_lines(facebook_row[\"phone\"]) if pd.notna(facebook_row[\"phone\"]) else \"\"\n",
    "    website_row[\"new_phone\"] = remove_plus_spaces_lines(website_row[\"phone\"]) if pd.notna(website_row[\"phone\"]) else \"\"\n",
    "    phone_number_freq = create_freq_dict(website_row, facebook_row, google_row, \"new_phone\", \"new_phone\", \"new_phone\")\n",
    "    name_freq = create_freq_dict(website_row, facebook_row, google_row, \"site_name\", \"name\", \"name\")\n",
    "\n",
    "    new_row[1] = max(country_freq, key=country_freq.get) if len(country_freq) != 0 else \"\"\n",
    "    new_row[2] = max(region_freq, key=region_freq.get) if len(region_freq) != 0 else \"\"\n",
    "    new_row[3] = max(phone_number_freq, key=phone_number_freq.get) if len(phone_number_freq) != 0 else \"\"\n",
    "\n",
    "    if google_row[\"new_phone\"] == new_row[3]:\n",
    "        new_row[3] = google_row[\"phone\"]\n",
    "    elif facebook_row[\"new_phone\"] == new_row[3]:\n",
    "        new_row[3] = facebook_row[\"phone\"]\n",
    "    elif website_row[\"new_phone\"] == new_row[3]:\n",
    "        new_row[3] = website_row[\"phone\"]\n",
    "\n",
    "    new_row[4] = max(name_freq, key=name_freq.get) if len(name_freq) != 0 else \"\"\n",
    "        \n",
    "    if pd.notna(google_row[\"name\"]):\n",
    "        if google_row[\"name\"].lower() == new_row[4]:\n",
    "            new_row[4] = google_row[\"name\"]\n",
    "        elif pd.notna(facebook_row[\"name\"]):\n",
    "            if facebook_row[\"name\"].lower() == new_row[4]:\n",
    "                new_row[4] = facebook_row[\"phone\"]\n",
    "            elif pd.notna(website_row[\"site_name\"]):\n",
    "                if website_row[\"site_name\"] == new_row[4]:\n",
    "                    new_row[4] = website_row[\"site_name\"]\n",
    "    \n",
    "    \n",
    "    return pd.concat([pd.DataFrame([new_row], columns=new_df.columns), new_df], ignore_index=True)\n",
    "\n",
    "def validate_google_rows(google_row, other_row, row_name, row_phone):\n",
    "    lambda_func = lambda x: validate_rows_are_similar(x, other_row, \"name\", row_name, \"phone\", row_phone)\n",
    "    validate_rows = google_row.apply(lambda_func, axis=1)\n",
    "    if len(validate_rows[validate_rows == 2]) != 0:\n",
    "        return google_row[validate_rows == 2].iloc[0]\n",
    "    elif len(validate_rows[validate_rows == 2]) != 0:\n",
    "        return google_row[validate_rows == 1].iloc[0]\n",
    "    else:\n",
    "        return dummy_google_df.iloc[0]\n",
    "\n",
    "def validate_rows_are_similar(row1, row2, row1_name, row2_name, row1_phone, row2_phone):\n",
    "    ok = 0\n",
    "    if pd.notna(row1[row1_phone]) and pd.notna(row2[row2_phone]):\n",
    "        if remove_plus_spaces_lines(row1[row1_phone]) == remove_plus_spaces_lines(row2[row2_phone]):\n",
    "            ok = 1\n",
    "    if pd.notna(row1[row1_name]) and pd.notna(row2[row2_name]):\n",
    "        if row1[row1_name] in row2[row2_name] or row2[row2_name] in row1[row1_name]:\n",
    "            ok = ok + 1\n",
    "\n",
    "    return ok\n",
    "\n",
    "new_new_df = new_df.copy()\n",
    "\n",
    "for el in duplicated_dom:\n",
    "    website_row = website_df[website_df[\"root_domain\"] == str(el)]\n",
    "    facebook_row = facebook_df[facebook_df[\"domain\"] == str(el)]\n",
    "    google_row = google_df[google_df[\"domain\"] == str(el)]\n",
    "    if len(website_row) == 0:\n",
    "        website_row = dummy_website_df\n",
    "    if len(google_row) == 0:\n",
    "        google_row = dummy_google_df\n",
    "    new_row = [\"\"] * 5 + [str(el)]\n",
    "\n",
    "    facebook_row = facebook_row.iloc[0]\n",
    "    website_row = website_row.iloc[0]\n",
    "    # having a public domain, it is possible that the website and facebook actually have different companies\n",
    "    # we will check them by a fairly simple assumption\n",
    "    if validate_rows_are_similar(facebook_row, website_row, \"name\", \"site_name\", \"phone\", \"phone\") > 0:\n",
    "        google_row = validate_google_rows(google_row, facebook_row, \"name\", \"phone\")\n",
    "        new_new_df = add_new_line(website_row, facebook_row, google_row, new_row, new_new_df)\n",
    "    else:\n",
    "        google_row_one = validate_google_rows(google_row, facebook_row, \"name\", \"phone\")\n",
    "        new_new_df = add_new_line(website_row, facebook_row, google_row_one, new_row, new_new_df)\n",
    "        google_row_two = validate_google_rows(google_row, website_row, \"site_name\", \"phone\")\n",
    "        new_new_df = add_new_line(website_row, facebook_row, google_row_two, new_row, new_new_df,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can save it (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_df.to_csv(\"new_merged_datasets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can say that we finished with two datasets to join: website (except the 13 rows with corrupted domain) and facebook. Now, the only thing to do is to take the google dataset and put all of the others which were not used in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_df = new_df.copy()\n",
    "for el in duplicated_dom:\n",
    "    \n",
    "    website_row = website_df[website_df[\"root_domain\"] == str(el)]\n",
    "    website_row = website_row.iloc[0] if len(website_row) != 0 else dummy_website_df.iloc[0]\n",
    "    facebook_row = facebook_df[facebook_df[\"domain\"] == str(el)].iloc[0]\n",
    "    google_row = google_df[google_df[\"domain\"] == str(el)]\n",
    "    \n",
    "    lambda_facebook_func = lambda x: validate_rows_are_similar(x, facebook_row, \"name\", \"name\", \"phone\", \"phone\")\n",
    "    lambda_website_func = lambda x: validate_rows_are_similar(x, website_row, \"name\", \"site_name\", \"phone\", \"phone\")\n",
    "\n",
    "    validate_facebook_rows = google_row.apply(lambda_facebook_func, axis=1)\n",
    "    validate_website_rows = google_row.apply(lambda_website_func, axis=1)\n",
    "    \n",
    "    no_common_facebook = validate_facebook_rows[validate_facebook_rows == 0].index.tolist()\n",
    "    no_common_website = validate_website_rows[validate_website_rows == 0].index.tolist()\n",
    "    merged_list = list(set(no_common_facebook + no_common_website))\n",
    "    for i in merged_list:\n",
    "        add_google_row = google_df.iloc[i]\n",
    "        # this line does not have anything in common with the other two datasets, so we can add it immediately\n",
    "        new_row = [\n",
    "            add_google_row[\"category\"],\n",
    "            add_google_row[\"country_name\"],\n",
    "            add_google_row[\"region_name\"],\n",
    "            add_google_row[\"phone\"],\n",
    "            add_google_row[\"name\"],\n",
    "            add_google_row[\"domain\"]\n",
    "        ]\n",
    "        \n",
    "        new_new_df = pd.concat([pd.DataFrame([new_row], columns=new_new_df.columns), new_new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save it again, again and again... (we save it so often in case we lose the jupyter session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_df.to_csv(\"new_new_merged_datasets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for having a final dataset/csv with only the columns wanted, we will delete domain column and save it again as \"final_merged_dataset.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = new_new_df.drop(columns=[\"Domain\"])\n",
    "final_df.to_csv(\"final_merged_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "1. in this notebook, we analysed  and merged three datasets: facebook, google and website csvs.\n",
    "2. we observed that: \n",
    "    - the names of the companies from the three datasets can differ with a word or two\n",
    "    - more companies can have the same phone number (probably because they have the same owner) and also the same names\n",
    "    - domain column is the only one which has unique elements in two datasets: \"facebook\" and \"website\", and also all of the elements from the other tables are non-null\n",
    "    - each element from facebook domain column can be found in the other two datasets, and vice-versa (except 13 elements from \"website\").\n",
    "    - 15000 of the elements are not unique for google dataset, and can be found at different companies, but nonetheless, all of them exists in the other tables (except \"website\", from those 13 elements) \n",
    "    - altough domain is a really good start, in the \"website\" table there are 13 data which seem to have really weird domain, which will be ignored\n",
    "3. The steps for joining all of the tables are:\n",
    "    1. Getting all of the domain elements from facebook (which are already unique)\n",
    "    2. Separating them into unique and duplicated from the perspective of google dataset\n",
    "    3. Solving the ones which are unique by taking all the elements from all the tables with the same domain, and create a majority vote for getting a name, phone number, country or region. If the vote is equal, the first one will be taken.\n",
    "    4. After that, we take the duplicated elements from google dataset and we will put the companies from facebook and website dataset, in the same manner, but adding a similiarity vote, which verify if the other company has at least a phone number or the name in common with the other, just to be sure that we do not put two times the same company and also for finding out which company from the google csv could be from the facebook csv or website csv\n",
    "    5. At last, we take the rest of the companies from google (which were not used and do not have a pair with the other datasets), and put them one by one in the new dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
